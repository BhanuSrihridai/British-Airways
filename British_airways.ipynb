{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhanuSrihridai/British-Airways/blob/Development/British_airways.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jLkusVaNZCIh"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNqUT4jkZCIj",
        "outputId": "04225a5d-e63e-4619-deb4-caee913ceaea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping page 1\n",
            "   ---> 100 total reviews\n",
            "Scraping page 2\n",
            "   ---> 200 total reviews\n",
            "Scraping page 3\n",
            "   ---> 300 total reviews\n",
            "Scraping page 4\n",
            "   ---> 400 total reviews\n",
            "Scraping page 5\n",
            "   ---> 500 total reviews\n",
            "Scraping page 6\n",
            "   ---> 600 total reviews\n",
            "Scraping page 7\n",
            "   ---> 700 total reviews\n",
            "Scraping page 8\n",
            "   ---> 800 total reviews\n",
            "Scraping page 9\n",
            "   ---> 900 total reviews\n",
            "Scraping page 10\n",
            "   ---> 1000 total reviews\n"
          ]
        }
      ],
      "source": [
        "base_url=\"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
        "pages=10\n",
        "page_size=100\n",
        "\n",
        "reviews=[]\n",
        "\n",
        "for i in range(1,pages+1):\n",
        "\n",
        "    print(f\"Scraping page {i}\")\n",
        "\n",
        "    # Creating url from a page to collect data\n",
        "    # url = https://www.airlinequality.com/airline-reviews/british-airways/page/2/?sortby=post_date%3ADesc&pagesize=100\n",
        "\n",
        "    url=f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
        "\n",
        "    # Collect data from this page\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse content\n",
        "    content = response.content\n",
        "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
        "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
        "        reviews.append(para.get_text())\n",
        "\n",
        "    print(f\"   ---> {len(reviews)} total reviews\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "liGqfGveZCIk",
        "outputId": "376fd747-5902-4232-d0fb-eedea0af05cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>✅ Trip Verified |  Busy day at LHR and flight ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>✅ Trip Verified |  Faro to Heathrow. Flight wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>✅ Trip Verified |  British Airways are in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632</th>\n",
              "      <td>✅ Trip Verified |  Warsaw to London. Everythin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>✅ Trip Verified |  Belfast City to Atlanta via...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>✅ Trip Verified |   The worst airline I have e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>✅ Trip Verified |   I am a frequent flyer with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>✅ Trip Verified |  Absolutely bad experience w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>✅ Trip Verified |  A short hop from London to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>Not Verified | Horrible airline. Does not care...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               reviews\n",
              "140  ✅ Trip Verified |  Busy day at LHR and flight ...\n",
              "887  ✅ Trip Verified |  Faro to Heathrow. Flight wa...\n",
              "419  ✅ Trip Verified |  British Airways are in the ...\n",
              "632  ✅ Trip Verified |  Warsaw to London. Everythin...\n",
              "856  ✅ Trip Verified |  Belfast City to Atlanta via...\n",
              "26   ✅ Trip Verified |   The worst airline I have e...\n",
              "19   ✅ Trip Verified |   I am a frequent flyer with...\n",
              "421  ✅ Trip Verified |  Absolutely bad experience w...\n",
              "567  ✅ Trip Verified |  A short hop from London to ...\n",
              "96   Not Verified | Horrible airline. Does not care..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame()\n",
        "df[\"reviews\"] = reviews\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEyOMS2IZCIk"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"C:\\Data Science\\Internship\\British Airways\\British Airways.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"British Airways.csv\")"
      ],
      "metadata": {
        "id": "j3JtTeMe65OY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuikA9otZCIk"
      },
      "source": [
        "Removing the punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ek1oss63ZCIl",
        "outputId": "90a67514-8dda-4ef2-e499-2ec562febf91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-91d5aada4604>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['review_processed'] = df['reviews'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n"
          ]
        }
      ],
      "source": [
        "df['review_processed'] = df['reviews'].str.replace(\"[^a-zA-Z0-9]\", \" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YjsI93YcZCIm",
        "outputId": "b20592d3-f522-4d5e-ba2b-65c3ca444a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Trip Verified    Booked online months ago an...\n",
              "1        Trip Verified    The flight was on time  The...\n",
              "2      Not Verified    Angry  disappointed  and unsat...\n",
              "3        Trip Verified    As an infrequent flyer  Bri...\n",
              "4      Not Verified    A totally unremarkable flight ...\n",
              "                             ...                        \n",
              "995      Trip Verified    San Francisco to London  Te...\n",
              "996      Trip Verified    Heathrow to Vancouver  The ...\n",
              "997      Trip Verified    London to Bucharest  First ...\n",
              "998      Trip Verified    I forgot I had purchased a ...\n",
              "999      Trip Verified    When the passenger in front...\n",
              "Name: review_processed, Length: 1000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df[\"review_processed\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IlU6eGZZCIm"
      },
      "source": [
        "Replacing the shorter words with space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "twyc_cJkZCIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64636a03-ecff-4c9b-fb8b-257eea9f6b4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      Trip Verified Booked online months ago and the...\n",
              "1      Trip Verified The flight was time The crew wer...\n",
              "2      Not Verified Angry disappointed and unsatisfie...\n",
              "3      Trip Verified infrequent flyer British Airways...\n",
              "4      Not Verified totally unremarkable flight time ...\n",
              "                             ...                        \n",
              "995    Trip Verified San Francisco London Terrible se...\n",
              "996    Trip Verified Heathrow Vancouver The seats boo...\n",
              "997    Trip Verified London Bucharest First class gro...\n",
              "998    Trip Verified forgot had purchased hand baggag...\n",
              "999    Trip Verified When the passenger front recline...\n",
              "Name: review_processed, Length: 1000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "df['review_processed']=df['review_processed'].apply(lambda row: ' '.join([word for word in row.split() if len(word)>2]))\n",
        "df['review_processed']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnzS8Y59ZCIn"
      },
      "source": [
        "Converting all words into lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "h0xJDPAWZCIn",
        "outputId": "8549eefa-9df7-4fa2-80c7-174d88837c09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      trip verified booked online months ago and the...\n",
              "1      trip verified the flight was time the crew wer...\n",
              "2      not verified angry disappointed and unsatisfie...\n",
              "3      trip verified infrequent flyer british airways...\n",
              "4      not verified totally unremarkable flight time ...\n",
              "                             ...                        \n",
              "995    trip verified san francisco london terrible se...\n",
              "996    trip verified heathrow vancouver the seats boo...\n",
              "997    trip verified london bucharest first class gro...\n",
              "998    trip verified forgot had purchased hand baggag...\n",
              "999    trip verified when the passenger front recline...\n",
              "Name: review_processed, Length: 1000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df['review_processed']=[row.lower() for row in df['review_processed']]\n",
        "df['review_processed']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS8ePrVAZCIp"
      },
      "source": [
        "Removing the stop words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words"
      ],
      "metadata": {
        "id": "n4d1eYCKZEE8",
        "outputId": "5cca71d7-33eb-4ae6-ac09-7331faf2e54c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "\n",
        "stop_words = stopwords.words('english') # extracting all the stop words in english language and storing it in a variable called stop_words -> set\n",
        "\n",
        "# Making custom list of words to be removed\n",
        "add_words = ['movie','film','one','make','even','see','movies','get','makes','making','time','watch','character', 'like', 'good','well','would','really']\n",
        "\n",
        "# Adding to the list of words\n",
        "stop_words.extend(add_words)\n",
        "\n",
        "# Function to remove stop words\n",
        "def remove_stopwords(rev):\n",
        "    # iNPUT : IT WILL TAKE ROW/REVIEW AS AN INPUT\n",
        "    # take the paragraph, break into words, check if the word is a stop word, remove if stop word, combine the words into a para again\n",
        "    review_tokenized = word_tokenize(rev)\n",
        "    rev_new = \" \".join([i for i in review_tokenized  if i not in stop_words])\n",
        "    return rev_new\n",
        "\n",
        "# Removing stopwords\n",
        "df['review_processed'] = [remove_stopwords(r) for r in df['review_processed']]"
      ],
      "metadata": {
        "id": "XMp7iaG95dwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(rev):\n",
        "    # input : IT WILL TAKE ROW/REVIEW AS AN INPUT\n",
        "    # take the paragraph, break into words, check if the word is a stop word, remove if stop word, combine the words into a para again\n",
        "    review_tokenized = word_tokenize(rev)\n",
        "    rev_new = \" \".join([i for i in review_tokenized  if i not in stop_words])\n",
        "    return rev_new"
      ],
      "metadata": {
        "id": "vcVuzBPb-dXv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review_processed']=[remove_stopwords(r) for r in df['review_processed']]"
      ],
      "metadata": {
        "id": "LrP-6c-n-ryc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review_processed']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFLndeJA_CPt",
        "outputId": "526e2ac9-a1a3-4d88-f840-a8953c28323c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      trip verified booked online months ago hitch r...\n",
              "1      trip verified flight time crew polite story ou...\n",
              "2      verified angry disappointed unsatisfied route ...\n",
              "3      trip verified infrequent flyer british airways...\n",
              "4      verified totally unremarkable flight time comf...\n",
              "                             ...                        \n",
              "995    trip verified san francisco london terrible se...\n",
              "996    trip verified heathrow vancouver seats booked ...\n",
              "997    trip verified london bucharest first class gro...\n",
              "998    trip verified forgot purchased hand baggage fa...\n",
              "999    trip verified passenger front reclines seat ma...\n",
              "Name: review_processed, Length: 1000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatisation"
      ],
      "metadata": {
        "id": "_GhV4ZAGAVTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOLwDlPuAWnG",
        "outputId": "8f594ea8-9e89-4305-bf54-e6c85a3daac4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatiser = WordNetLemmatizer()\n",
        "\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "\n",
        "  elif nltk_tag.startswith('N'):\n",
        "      return wordnet.NOUN\n",
        "\n",
        "  elif nltk_tag.startswith('R'):\n",
        "      return wordnet.ADV\n",
        "  else:\n",
        "      return None\n"
      ],
      "metadata": {
        "id": "S_EJt431EOvn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_sentence(sentence):\n",
        "\n",
        "  nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "\n",
        "  wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "\n",
        "  lemmatized_sentence = []\n",
        "\n",
        "  for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatiser.lemmatize(word, tag))\n",
        "\n",
        "  return \" \".join(lemmatized_sentence)\n",
        "\n",
        "df['review_processed'] = df['review_processed'].apply(lambda x: lemmatize_sentence(x))\n"
      ],
      "metadata": {
        "id": "uXk3o6pX5z0R"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review_processed']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97c1nHFe6S9L",
        "outputId": "f6093cb6-dfa8-493a-87ed-56e9c6c5dba5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      trip verify booked online month ago hitch repl...\n",
              "1      trip verify flight time crew polite story outw...\n",
              "2      verified angry disappoint unsatisfied route lo...\n",
              "3      trip verify infrequent flyer british airway al...\n",
              "4      verify totally unremarkable flight time comfor...\n",
              "                             ...                        \n",
              "995    trip verify san francisco london terrible serv...\n",
              "996    trip verify heathrow vancouver seat book give ...\n",
              "997    trip verify london bucharest first class groun...\n",
              "998    trip verify forgot purchase hand baggage fare ...\n",
              "999    trip verify passenger front reclines seat make...\n",
              "Name: review_processed, Length: 1000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}